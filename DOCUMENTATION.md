#  H·ªá Th·ªëng Gi√°m S√°t T√†i X·∫ø (DMS) - Documentation

##  M·ª•c L·ª•c

1. [T·ªïng Quan](#t·ªïng-quan)
2. [Y√™u C·∫ßu H·ªá Th·ªëng](#y√™u-c·∫ßu-h·ªá-th·ªëng)
3. [C√†i ƒê·∫∑t & Ch·∫°y](#c√†i-ƒë·∫∑t--ch·∫°y)
4. [Ki·∫øn Tr√∫c H·ªá Th·ªëng](#ki·∫øn-tr√∫c-h·ªá-th·ªëng)
5. [API Reference](#api-reference)
6. [Configuration](#configuration)
7. [Thu·∫≠t To√°n](#thu·∫≠t-to√°n)
8. [V√≠ D·ª• S·ª≠ D·ª•ng](#v√≠-d·ª•-s·ª≠-d·ª•ng)
9. [Troubleshooting](#troubleshooting)

---

##  T·ªïng Quan

**DMS (Driver Monitoring System)** l√† h·ªá th·ªëng th·ªùi gian th·ª±c ph√°t hi·ªán:
-  **Bu·ªìn ng·ªß** - Theo d√µi nh·∫Øm m·∫Øt li√™n t·ª•c
-  **Ng√°p** - Ph√°t hi·ªán mi·ªáng m·ªü l·ªõn
-  **T∆∞ th·∫ø ƒë·∫ßu** - H∆∞·ªõng nh√¨n kh√¥ng ƒë√∫ng
-  **M·∫•t t·∫≠p trung** - Tay g·∫ßn m·∫∑t (d√πng ƒëi·ªán tho·∫°i)

**C√¥ng ngh·ªá:**
- OpenCV: X·ª≠ l√Ω h√¨nh ·∫£nh
- MediaPipe: Ph√°t hi·ªán khu√¥n m·∫∑t/tay
- NumPy: T√≠nh to√°n s·ªë h·ªçc
- One-Euro Filter: L√†m m·ªãn t√≠n hi·ªáu

---

##  Y√™u C·∫ßu H·ªá Th·ªëng

### Ph·∫ßn C·ª©ng
```
Processor: Intel i5+ ho·∫∑c equivalent
RAM: 4GB minimum (8GB recommended)
Webcam: USB camera ho·∫∑c built-in
GPU: Kh√¥ng b·∫Øt bu·ªôc (s·ª≠ d·ª•ng CPU)
```

### Ph·∫ßn M·ªÅm
```
Python: 3.9+
OS: Windows 10+, Ubuntu 18+, macOS 10.14+
```

### Dependencies
```
opencv-python>=4.8.0
mediapipe>=0.10.0
numpy>=1.24.0
playsound>=1.2.2 (t√πy ch·ªçn, cho c·∫£nh b√°o √¢m thanh)
```

---

##  C√†i ƒê·∫∑t & Ch·∫°y

### 1. Clone/Download D·ª± √Ån
```bash
cd c:\Users\Quang Minh\OneDrive\PROJECT-22
```

### 2. C√†i ƒê·∫∑t Dependencies
```bash
pip install -r requirements.txt
```

### 3. Ch·∫°y ·ª®ng D·ª•ng
```bash
# V·ªõi camera m·∫∑c ƒë·ªãnh (ID = 0)
python main.py

# V·ªõi camera kh√°c (ID = 1)
python main.py --camera 1

# V·ªõi resolution kh√°c
python main.py --width 1280 --height 720

# T·∫•t c·∫£ t√πy ch·ªçn
python main.py --camera 0 --width 640 --height 480
```

### 4. D·ª´ng ·ª®ng D·ª•ng
Nh·∫•n ph√≠m `q` tr√™n c·ª≠a s·ªï video

---

##  Ki·∫øn Tr√∫c H·ªá Th·ªëng

### S∆° ƒê·ªì Flow

```
Input (Camera)
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TienXuLyCLAHE              ‚îÇ ‚Üê TƒÉng s√°ng ·∫£nh
‚îÇ  (Preprocessing)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Ph√¢n T√≠ch Song Song         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ PhanTichMat           TheoDoiTay    ‚îÇ
‚îÇ (Face Analysis)       (Hand Track)  ‚îÇ
‚îÇ ‚îú‚îÄ EAR (bu·ªìn ng·ªß)     ‚îú‚îÄ Ph√°t hi·ªán ‚îÇ
‚îÇ ‚îú‚îÄ MAR (ng√°p)         ‚îî‚îÄ V·ªã tr√≠    ‚îÇ
‚îÇ ‚îî‚îÄ T∆∞ th·∫ø ƒë·∫ßu                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BoLocOneEuro/              ‚îÇ ‚Üê L√†m m·ªãn
‚îÇ  BoLocOneEuroNhieuKenh      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TraoDuaTinhNang            ‚îÇ ‚Üê Hi·ªÉn th·ªã
‚îÇ  (Visualization)            ‚îÇ   + C·∫£nh b√°o
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üì
Output (Screen) + Alert Sound
```

### Module Structure

```
dms/
‚îú‚îÄ‚îÄ constants.py          # C·∫•u h√¨nh + ng∆∞·ª°ng
‚îú‚îÄ‚îÄ preprocessing.py      # CLAHE (tƒÉng s√°ng)
‚îú‚îÄ‚îÄ filters.py            # One-Euro filter
‚îú‚îÄ‚îÄ face_analysis.py      # EAR, MAR, Head Pose
‚îú‚îÄ‚îÄ hand_tracking.py      # Ph√°t hi·ªán tay
‚îú‚îÄ‚îÄ visualization.py      # V·∫Ω + hi·ªÉn th·ªã
‚îî‚îÄ‚îÄ __init__.py          # Kh·ªüi t·∫°o package

main.py                   # ƒêi·ªÉm ch√≠nh
```

---

## üîå API Reference

### PhanTichMat (Face Analysis)

```python
from dms.face_analysis import PhanTichMat

# Kh·ªüi t·∫°o
analyzer = PhanTichMat(
    so_mat_toi_da=1,              # Max faces to detect
    do_tin_cay_phat_hien=0.5,     # Detection confidence
    do_tin_cay_theo_doi=0.5,      # Tracking confidence
    tinh_toan_diem_chi_tiet=True  # Refine landmarks
)

# Ph√¢n t√≠ch
result = analyzer.analyze(frame, timestamp=None)

# Output
{
    'mat_phat_hien': bool,           # M·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán?
    'ear': float,                     # Eye Aspect Ratio
    'mar': float,                     # Mouth Aspect Ratio
    'pitch': float,                   # G√≥c pitch (ƒë·ªô)
    'yaw': float,                     # G√≥c yaw (ƒë·ªô)
    'roll': float,                    # G√≥c roll (ƒë·ªô)
    'canh_bao_buon_ngu': bool,       # Bu·ªìn ng·ªß?
    'canh_bao_ngap': bool,            # Ng√°p?
    'canh_bao_tu_the': bool,         # T∆∞ th·∫ø sai?
    'diem_moc': list,                 # 468 landmarks
    'diem_moc_mat': dict,             # {'phai': [...], 'trai': [...]}
    'diem_moc_mieng': list,           # Mouth landmarks
    'vec_quay': ndarray,              # Rotation vector
    'vec_tuan': ndarray,              # Translation vector
    'khung_bbox_mat': dict            # {'x_min', 'x_max', 'y_min', 'y_max'}
}

# Gi·∫£i ph√≥ng
analyzer.release()
```

### TheoDoiTay (Hand Tracking)

```python
from dms.hand_tracking import TheoDoiTay

# Kh·ªüi t·∫°o
tracker = TheoDoiTay(
    so_tay_toi_da=2,                # Max hands
    do_tin_cay_phat_hien=0.5,
    do_tin_cay_theo_doi=0.5
)

# Ph√¢n t√≠ch
result = tracker.analyze(frame, khung_bbox_mat)

# Output
{
    'hands_detected': int,           # S·ªë tay ph√°t hi·ªán
    'hand_landmarks': list,          # Landmarks m·ªói tay
    'hand_bboxes': list,             # Bounding box m·ªói tay
    'hand_near_face': bool,          # Tay g·∫ßn m·∫∑t?
    'distraction_alert': bool,       # M·∫•t t·∫≠p trung?
    'distraction_duration': float    # Th·ªùi gian (gi√¢y)
}

tracker.release()
```

### TienXuLyCLAHE (Preprocessing)

```python
from dms.preprocessing import TienXuLyCLAHE

# Kh·ªüi t·∫°o
preprocessor = TienXuLyCLAHE(
    han_clip=2.0,              # Clip limit
    kich_thuoc_o=(8, 8),       # Tile grid size
    khong_gian_mau='YCRCB'     # 'YCRCB' ho·∫∑c 'LAB'
)

# X·ª≠ l√Ω
enhanced_frame = preprocessor.tang_cuong(frame)
```

### BoLocOneEuro (Filter)

```python
from dms.filters import BoLocOneEuro

# Kh·ªüi t·∫°o
filter = BoLocOneEuro(
    cutoff_toi_thieu=1.0,  # Min cutoff
    beta=0.007,             # Movement sensitivity
    cutoff_dao_ham=1.0     # Derivative cutoff
)

# L·ªçc gi√° tr·ªã
smoothed = filter.loc(raw_value, timestamp)
```

---

## ‚öôÔ∏è Configuration

### S·ª≠a ƒê·ªïi Ng∆∞·ª°ng

Ch·ªânh s·ª≠a `dms/constants.py`:

```python
@dataclass(frozen=True, slots=True)
class DrowsinessConfig:
    nguong_ear: float = 0.2      # ‚Üê Gi·∫£m = nh·∫°y h∆°n
    so_khung_ear: int = 15       # ‚Üê TƒÉng = c·∫ßn bu·ªìn ng·ªß l√¢u h∆°n
    nguong_mar: float = 1.3
    thoi_gian_canh_bao_am_thanh: float = 5.0  # ‚Üê Th·ªùi gian tr∆∞·ªõc c·∫£nh b√°o
    khoang_cach_am_thanh: float = 2.0         # ‚Üê Cooldown gi·ªØa c·∫£nh b√°o
```

### S·ª≠a ƒê·ªïi File √Çm Thanh

Trong `main.py`:

```python
@dataclass
class HeThongGiamSatTaiXe:
    duong_dan_am_thanh: str = "chicken-on-tree.mp3"  # ‚Üê ƒê·ªïi ƒë∆∞·ªùng d·∫´n
```

---

## üß† Thu·∫≠t To√°n

### 1. Eye Aspect Ratio (EAR)

**C√¥ng th·ª©c:**
```
EAR = (||p2 - p6|| + ||p3 - p5||) / (2 * ||p1 - p4||)

p1-p6: 6 landmarks m·∫Øt
```

**√ù nghƒ©a:**
- EAR ‚âà 0.2 ‚Üí M·∫Øt nh·∫Øm
- EAR > 0.2 ‚Üí M·∫Øt m·ªü

**Tham kh·∫£o:** Tereza et al., 2016

### 2. Mouth Aspect Ratio (MAR)

**C√¥ng th·ª©c:**
```
MAR = (||p2 - p8|| + ||p3 - p7|| + ||p4 - p6||) / (2 * ||p1 - p5||)
```

**√ù nghƒ©a:**
- MAR > 1.3 ‚Üí Ng√°p

### 3. Head Pose Estimation

**Ph∆∞∆°ng ph√°p:** solvePnP (Perspective-n-Point)

```
Input:
  - 3D model points (6 ƒëi·ªÉm m·∫∑t chu·∫©n)
  - 2D image points (6 landmarks tr√™n ·∫£nh)
  - Camera matrix K

Output:
  - Rotation vector (rvec) ‚Üí Euler angles (pitch, yaw, roll)
  - Translation vector (tvec) ‚Üí V·ªã tr√≠ ƒë·∫ßu
```

**Thresholds:**
- Pitch > 20¬∞ ‚Üí Nh√¨n xu·ªëng/l√™n
- Yaw > 30¬∞ ‚Üí Nh√¨n sang
- Roll > 25¬∞ ‚Üí Nghi√™ng ƒë·∫ßu

### 4. One-Euro Filter

**Adaptive smoothing:**
```
fc(t) = f_min + Œ≤ * |dx(t)|

Œ±(f) = 1 / (1 + œÑ / te)
```

**L·ª£i √≠ch:**
- Ch·∫≠m ‚Üí M·ªãn m·∫°nh
- Nhanh ‚Üí M·ªãn nh·∫π
- Adaptive ‚Üí C√¢n b·∫±ng gi·ªØa lag v√† noise

**Tham kh·∫£o:** Casiez et al., CHI 2012

### 5. CLAHE (Contrast Limited Adaptive Histogram Equalization)

**T√°c d·ª•ng:**
- X·ª≠ l√Ω c·ª•c b·ªô (local) thay v√¨ to√†n c·ª•c
- Gi·ªõi h·∫°n contrast ‚Üí Tr√°nh artifacts
- T·ªët cho ·∫£nh √°nh s√°ng y·∫øu

---

## üí° V√≠ D·ª• S·ª≠ D·ª•ng

### V√≠ D·ª• 1: S·ª≠ D·ª•ng Tr·ª±c Ti·∫øp

```python
import cv2
from dms.face_analysis import PhanTichMat
from dms.preprocessing import TienXuLyCLAHE

# Kh·ªüi t·∫°o
preprocessor = TienXuLyCLAHE()
analyzer = PhanTichMat()

# M·ªü camera
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # X·ª≠ l√Ω
    enhanced = preprocessor.tang_cuong(frame)
    result = analyzer.analyze(enhanced)
    
    # Ki·ªÉm tra k·∫øt qu·∫£
    if result['canh_bao_buon_ngu']:
        print(" Ph√°t hi·ªán bu·ªìn ng·ªß!")
        print(f"EAR: {result['ear']:.2f}")
    
    if result['canh_bao_ngap']:
        print(" Ph√°t hi·ªán ng√°p!")
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

analyzer.release()
cap.release()
cv2.destroyAllWindows()
```

### V√≠ D·ª• 2: X·ª≠ L√Ω File Video

```python
import cv2
import time
from dms.face_analysis import PhanTichMat

analyzer = PhanTichMat()
cap = cv2.VideoCapture('video.mp4')

# L∆∞u k·∫øt qu·∫£
results = []

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    ts = time.time()
    result = analyzer.analyze(frame, ts)
    
    results.append({
        'timestamp': ts,
        'ear': result['ear'],
        'drowsiness': result['canh_bao_buon_ngu']
    })

# Ph√¢n t√≠ch
drowsy_frames = [r for r in results if r['drowsiness']]
drowsy_percent = len(drowsy_frames) / len(results) * 100
print(f"Bu·ªìn ng·ªß: {drowsy_percent:.1f}%")

analyzer.release()
cap.release()
```

### V√≠ D·ª• 3: T√πy Ch·ªânh Config

```python
import cv2
from dms.preprocessing import TienXuLyCLAHE
from dms.preprocessing import KhongGianMau

# CLAHE v·ªõi LAB color space (ch√≠nh x√°c h∆°n)
preprocessor = TienXuLyCLAHE(
    han_clip=4.0,           # TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
    khong_gian_mau=KhongGianMau.LAB
)

# X·ª≠ l√Ω
frame = cv2.imread('image.jpg')
enhanced = preprocessor.tang_cuong(frame)
```

---

## üîß Troubleshooting

### L·ªói: "Kh√¥ng th·ªÉ m·ªü camera"

**Nguy√™n nh√¢n:**
- Camera b·ªã chi·∫øm d·ª•ng b·ªüi ·ª©ng d·ª•ng kh√°c
- Driver camera kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t
- ID camera sai

**Gi·∫£i ph√°p:**
```bash
# Th·ª≠ camera ID kh√°c
python main.py --camera 1

# Ki·ªÉm tra camera available (Windows)
ffmpeg -list_devices true -f dshow -i dummy

# Ubuntu
ls /dev/video*
```

### L·ªói: "playsound not installed"

**Gi·∫£i ph√°p:**
```bash
pip install playsound
```

### Performance ch·∫≠m

**Nguy√™n nh√¢n:**
- CPU y·∫øu
- Resolution qu√° cao
- Nhi·ªÅu background processes

**Gi·∫£i ph√°p:**
```bash
# Gi·∫£m resolution
python main.py --width 320 --height 240

# Ho·∫∑c tƒÉng kho·∫£ng c√°ch CLAHE tile
# Ch·ªânh s·ª≠a dms/constants.py:
# kich_thuoc_o: Tuple[int, int] = (16, 16)  # TƒÉng t·ª´ 8, 8
```

### Ph√°t hi·ªán kh√¥ng ch√≠nh x√°c

**Nguy√™n nh√¢n:**
- √Ånh s√°ng y·∫øu
- G√≥c camera x·∫•u
- Khu√¥n m·∫∑t b·ªã che ph·ªß

**Gi·∫£i ph√°p:**
```
1. Ch·ªânh s√°ng (t·ªëi nh·∫•t 50 lux)
2. Camera ph√≠a tr∆∞·ªõc, ngang m·∫∑t
3. L√†m s·∫°ch lens
4. ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng trong constants.py
```

### C·∫£nh b√°o √¢m thanh kh√¥ng ph√°t

**Ki·ªÉm tra:**
```python
# Test ph√°t √¢m thanh
import playsound
playsound.playsound('chicken-on-tree.mp3')

# Ki·ªÉm tra volume h·ªá th·ªëng
# Windows: Volume mixer
# Linux: alsamixer
```

---

## Performance Metrics

### Y√™u c·∫ßu

| Kh√≠a c·∫°nh | Y√™u c·∫ßu |
|----------|---------|
| Frame Rate | 25-30 FPS |
| Latency | <100ms |
| Memory | <500MB |
| CPU | 40-60% (i5+) |

### Optimization Tips

```python
# 1. Gi·∫£m resolution
python main.py --width 320 --height 240

# 2. Skip frames
if frame_count % 2 == 0:  # Process every 2nd frame
    result = analyzer.analyze(frame)

# 3. Async processing
import threading
thread = threading.Thread(target=analyzer.analyze, args=(frame,))
thread.start()
```

---

## Tham Kh·∫£o Khoa H·ªçc

1. **EAR/MAR:**
   - Tereza et al., "Real Time Eye Gaze Tracking with 3D Deformable Eye-Face Model", ICCV 2015

2. **Head Pose:**
   - Lepetit & Fua, "Keypoint Recognition using Randomized Trees", TPAMI 2006

3. **One-Euro Filter:**
   - Casiez et al., "1‚Ç¨ Filter: A Simple Speed-based Low-pass Filter for Noisy Input in Interactive Systems", CHI 2012

4. **CLAHE:**
   - Zuiderveld, "Contrast Limited Adaptive Histogram Equalization", Graphics Gems IV, 1994

---

## H·ªó Tr·ª£

G·∫∑p v·∫•n ƒë·ªÅ?

1. Ki·ªÉm tra **Troubleshooting** section
2. Xem **log messages** (terminal)
3. Th·ª≠ gi·∫£m resolution ho·∫∑c ƒëi·ªÅu ch·ªânh thresholds
4. Li√™n h·ªá: minhdev@duck.com

---

**Version:** 1.0.0  
**Last Updated:** 2026-01-29  
**License:** MIT
